{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36042e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import common_utils as utils\n",
    "print(utils.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5d30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))\n",
    "     ])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.FER2013(root='./', split=\"train\",\n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.FER2013(root='./', split=\"test\",\n",
    "                                       transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd24908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = utils.BaseModel().to(utils.device)\n",
    "compiled_model = utils.BaseModel().to(utils.device)\n",
    "compiled_model.compile()\n",
    "benchmarkloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9064d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images Processed :  7178\n",
      "Total Time :  456.1165783405304 seconds\n",
      "Average Time:  0.06354368603239487 seconds\n",
      "Average FPS:  15.737204786801247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x117cc8e00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/adejuwon/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/adejuwon/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1627, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/adejuwon/miniconda3/envs/HPML_Final/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/adejuwon/miniconda3/envs/HPML_Final/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/adejuwon/miniconda3/envs/HPML_Final/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/adejuwon/miniconda3/envs/HPML_Final/lib/python3.12/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/adejuwon/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 5652) is killed by signal: Abort trap: 6. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m utils.benchmark_model(base_model, benchmarkloader)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbenchmark_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiled_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmarkloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Columbia School Junk/Fall 2025/High Performance Machine Learning/Final Project/common_utils.py:92\u001b[39m, in \u001b[36mbenchmark_model\u001b[39m\u001b[34m(model, dataloader, max_samples)\u001b[39m\n\u001b[32m     90\u001b[39m labels = labels.to(device)\n\u001b[32m     91\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[32m     93\u001b[39m end_time = time.time()    \n\u001b[32m     94\u001b[39m total += labels.size(\u001b[32m0\u001b[39m)    \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/nn/modules/module.py:1749\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compiled_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:655\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    652\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Columbia School Junk/Fall 2025/High Performance Machine Learning/Final Project/common_utils.py:39\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mself\u001b[39m.fc3 = nn.Linear(\u001b[32m1024\u001b[39m, \u001b[32m256\u001b[39m)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mself\u001b[39m.fc4 = nn.Linear(\u001b[32m256\u001b[39m, \u001b[32m7\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     40\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool(F.relu(\u001b[38;5;28mself\u001b[39m.conv1(x)))\n\u001b[32m     41\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool(F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1209\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1207\u001b[39m full_args.extend(params_flat)\n\u001b[32m   1208\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:328\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n\u001b[32m    327\u001b[39m         torch._C._set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:689\u001b[39m, in \u001b[36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    686\u001b[39m     args = [*([\u001b[38;5;28;01mNone\u001b[39;00m] * num_tokens), *args]\n\u001b[32m    687\u001b[39m     old_args.clear()\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m outs = \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:495\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    488\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    489\u001b[39m         runtime_metadata,\n\u001b[32m    490\u001b[39m         out,\n\u001b[32m    491\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    492\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    493\u001b[39m     )\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/_inductor/output_code.py:460\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    462\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/05/r_3mxvgd2pl4dm6_3y9ty2z00000gn/T/torchinductor_adejuwon/nf/cnf5jgzde2hz7ezwkavrjolrqoug5tbsueoqy57z4lhmbbnhvehr.py:482\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    480\u001b[39m buf10 = reinterpret_tensor(buf7, (\u001b[32m1\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m6\u001b[39m), (\u001b[32m36864\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m6144\u001b[39m, \u001b[32m1024\u001b[39m), \u001b[32m0\u001b[39m); \u001b[38;5;28;01mdel\u001b[39;00m buf7  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[32m    481\u001b[39m buf11 = empty_strided_cpu((\u001b[32m2048\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m), (\u001b[32m9216\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3072\u001b[39m, \u001b[32m1024\u001b[39m), torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[43mcpp_fused_convolution_max_pool2d_with_indices_relu_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg9_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf11\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m arg9_1\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m buf9\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "utils.benchmark_model(base_model, benchmarkloader)\n",
    "utils.benchmark_model(compiled_model, benchmarkloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374a0dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0617,  0.0064, -0.0084,  0.0445,  0.0497,  0.0604, -0.0254],\n",
       "        [ 0.0618,  0.0063, -0.0084,  0.0447,  0.0498,  0.0605, -0.0253],\n",
       "        [ 0.0617,  0.0062, -0.0084,  0.0447,  0.0497,  0.0604, -0.0253],\n",
       "        [ 0.0617,  0.0062, -0.0084,  0.0447,  0.0497,  0.0605, -0.0254],\n",
       "        [ 0.0618,  0.0062, -0.0085,  0.0446,  0.0498,  0.0605, -0.0253],\n",
       "        [ 0.0618,  0.0063, -0.0084,  0.0446,  0.0497,  0.0605, -0.0254],\n",
       "        [ 0.0617,  0.0063, -0.0083,  0.0446,  0.0497,  0.0604, -0.0253],\n",
       "        [ 0.0617,  0.0062, -0.0085,  0.0446,  0.0497,  0.0604, -0.0253],\n",
       "        [ 0.0617,  0.0062, -0.0084,  0.0447,  0.0496,  0.0604, -0.0253],\n",
       "        [ 0.0618,  0.0063, -0.0085,  0.0447,  0.0498,  0.0606, -0.0254],\n",
       "        [ 0.0617,  0.0063, -0.0084,  0.0447,  0.0496,  0.0605, -0.0253],\n",
       "        [ 0.0617,  0.0064, -0.0085,  0.0446,  0.0498,  0.0604, -0.0254],\n",
       "        [ 0.0618,  0.0063, -0.0084,  0.0446,  0.0497,  0.0605, -0.0253],\n",
       "        [ 0.0617,  0.0063, -0.0084,  0.0447,  0.0497,  0.0605, -0.0253],\n",
       "        [ 0.0617,  0.0063, -0.0084,  0.0446,  0.0497,  0.0605, -0.0254],\n",
       "        [ 0.0617,  0.0063, -0.0084,  0.0446,  0.0497,  0.0605, -0.0253]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(iter(testloader))\n",
    "inputs = inputs.to(utils.device)\n",
    "labels = labels.to(utils.device)\n",
    "base_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7d7f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "\n",
      "        Train Loss: 0.12144458369902464\n",
      "        Train Acc: 13.91549688251071\n",
      "        Test Loss: 0.12155489200468214\n",
      "        Test Acc: 13.346336026748398    \n",
      "        \n",
      "[1,  1000] loss: 1.841\n",
      "Epoch 1 Finished\n",
      "\n",
      "            Train Loss: 0.11257903862989428\n",
      "            Train Acc: 25.13149186666202\n",
      "            Test Loss: 0.1127350349451513\n",
      "            Test Acc: 24.71440512677626\n",
      "            \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "utils.train(None, base_model, trainloader, testloader, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPML_Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
