{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dcae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import common_utils as utils\n",
    "import wandb\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n",
    "print(utils.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))\n",
    "     ])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.FER2013(root='./', split=\"train\",\n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.FER2013(root='./', split=\"test\",\n",
    "                                       transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False, num_workers=8)\n",
    "# Use a batchsize of 1 to more accurately model individual frames from camera\n",
    "benchmark_loader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccababd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you change these if you choose a different checkpoint!!\n",
    "base_model = utils.BaseModel()\n",
    "base_model.load_state_dict(torch.load(\"base_model/checkpoint_6.pth\", map_location=utils.device))\n",
    "base_model.to(utils.device)\n",
    "pruned_model = torch.load(\"pruned_model/checkpoint_1.pth\", weights_only=False, map_location=utils.device)\n",
    "pruned_model.to(utils.device)\n",
    "\n",
    "# Just requantize the model instead of loading jit tracse/state dicts.  Time saving\n",
    "# is minimal all things considered.\n",
    "quantized_base_model = utils.quantize_model(base_model, testloader)\n",
    "quantized_pruned_model = utils.quantize_model(pruned_model, testloader)\n",
    "\n",
    "compiled_base_model = copy.deepcopy(base_model)\n",
    "compiled_base_model.compile()\n",
    "compiled_pruned_model = copy.deepcopy(pruned_model)\n",
    "compiled_pruned_model.compile()\n",
    "compiled_quantized_base_model = copy.deepcopy(quantized_base_model)\n",
    "compiled_quantized_base_model.compile()\n",
    "compiled_quantized_pruned_model = copy.deepcopy(quantized_pruned_model)\n",
    "compiled_quantized_pruned_model.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d001d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"base_model\", base_model),\n",
    "    (\"pruned_model\", pruned_model),\n",
    "    (\"quantized_base_model\", quantized_base_model),\n",
    "    (\"quantized_pruned_model\", quantized_pruned_model),\n",
    "    (\"compiled_base_model\", compiled_base_model),\n",
    "    (\"compiled_pruned_model\", compiled_pruned_model),\n",
    "    (\"compiled_quantized_base_model\", compiled_quantized_base_model),\n",
    "    (\"compiled_quantized_pruned_model\", compiled_quantized_pruned_model),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = defaultdict(dict)\n",
    "for (name, model) in models:\n",
    "    run = wandb.init(project=\"hpml-final\", name=\"{0} Benchmark\".format(name))\n",
    "    loss, acc = utils.test(model, testloader)\n",
    "    benchmark_results[name][\"test_loss\"] = loss\n",
    "    benchmark_results[name][\"test_acc\"] = acc\n",
    "    utils.benchmark_model(model, benchmark_loader, 200) # warm start\n",
    "    benchmark_results[name][\"benchmark\"] = utils.benchmark_model(model, benchmark_loader, 200)\n",
    "    run.log(\n",
    "        {\n",
    "            \"test/loss\" : loss,\n",
    "            \"test/acc\" : acc,\n",
    "            \"benchmark/images\" : benchmark_results[name][\"benchmark\"][\"total_images\"],\n",
    "            \"benchmark/total_time\" : benchmark_results[name][\"benchmark\"][\"total_time\"],\n",
    "            \"benchmark/mean_time\" : benchmark_results[name][\"benchmark\"][\"mean_time\"],\n",
    "            \"benchmark/mean_fps\" : benchmark_results[name][\"benchmark\"][\"mean_fps\"],\n",
    "        }\n",
    "    )\n",
    "    run.finish()\n",
    "with open(\"benchmark_results.pkl\", \"wb\") as file:\n",
    "    pickle.dump(benchmark_results, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"benchmark_results.pkl\", \"rb\") as file:\n",
    "    benchmark_results = pickle.load(file)\n",
    "# fig, axs = plt.subplots(1, 4, figsize=(30, 12))\n",
    "plt.figure(figsize=(10, 6))\n",
    "key_order = [\n",
    "    \"base_model\",\n",
    "    \"quantized_base_model\",\n",
    "    \"pruned_model\",\n",
    "    \"quantized_pruned_model\",\n",
    "    \"compiled_base_model\",\n",
    "    \"compiled_quantized_base_model\",\n",
    "    \"compiled_pruned_model\",\n",
    "    \"compiled_quantized_pruned_model\"\n",
    "]\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "mean_times = []\n",
    "mean_frames_per_second = []\n",
    "\n",
    "for key in key_order:\n",
    "    test_losses.append(benchmark_results[key][\"test_loss\"])\n",
    "    test_accuracies.append(benchmark_results[key][\"test_acc\"])\n",
    "    mean_times.append(benchmark_results[key][\"benchmark\"][\"mean_time\"])\n",
    "    mean_frames_per_second.append(benchmark_results[key][\"benchmark\"][\"mean_fps\"])\n",
    "\n",
    "\n",
    "plt.plot(key_order[:4], test_accuracies[:4])\n",
    "plt.title(\"Test Accuracies Per Model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/benchmark_accuracies.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(key_order[:4], test_losses[:4])\n",
    "plt.title(\"Test Loss Per Model\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/benchmark_losses.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(key_order[:4], mean_times[:4], label=\"Uncompiled\")\n",
    "plt.plot(key_order[:4], mean_times[4:], label=\"Compiled\")\n",
    "plt.title(\"Mean Inference Time Per Model\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/benchmark_inference_times.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(key_order[:4], mean_frames_per_second[:4],label=\"Uncompiled\")\n",
    "plt.plot(key_order[:4], mean_frames_per_second[4:], label=\"Compiled\")\n",
    "plt.title(\"Mean FPS(estimated) Per Model\")\n",
    "plt.ylabel(\"FPS\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/benchmark_fps.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acdbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, model) in models: # do this after because we can't pickle these apparently\n",
    "    benchmark_results[name][\"profile\"] = utils.profile_model(model, benchmark_loader, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd12362",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"profile_results.txt\", \"w\") as file:\n",
    "    for (name, model) in models:\n",
    "        file.write(\"*************************\\n\")\n",
    "        file.write(name + \"\\n\")\n",
    "        file.write(\"*************************\\n\")\n",
    "        file.write(benchmark_results[name][\"profile\"].key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=30))\n",
    "        file.write(benchmark_results[name][\"profile\"].key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=30))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faecc488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPML_Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
