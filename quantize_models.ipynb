{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc18132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import common_utils as utils\n",
    "import os\n",
    "print(utils.device)\n",
    "utils.device = \"cpu\" # For quantization we use cpu\n",
    "print(utils.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4503e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))\n",
    "     ])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.FER2013(root='./', split=\"train\",\n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.FER2013(root='./', split=\"test\",\n",
    "                                       transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa518b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(2048, 4192, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=4192, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"pruned_model/checkpoint_1.pth\", \"rb\") as file:\n",
    "pruned_model = torch.load(\"pruned_model/checkpoint_1.pth\", weights_only=False, map_location=utils.device)\n",
    "pruned_model.to(utils.device)\n",
    "\n",
    "base_model = utils.BaseModel()\n",
    "base_model.load_state_dict(torch.load(\"base_model/checkpoint_6.pth\", map_location=utils.device))\n",
    "base_model.to(utils.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b54e19fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adejuwon/miniconda3/envs/HPML_Final/lib/python3.12/site-packages/torch/ao/quantization/observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "quantized_base_model = utils.quantize_model(base_model, testloader)\n",
    "quantized_pruned_model = utils.quantize_model(pruned_model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403c5a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images Processed :  112\n",
      "Total Time :  4.387948274612427 seconds\n",
      "Average Time:  0.03917810959475381 seconds\n",
      "Average FPS:  25.5244576714826\n",
      "Total Images Processed :  112\n",
      "Total Time :  1.053718090057373 seconds\n",
      "Average Time:  0.009408197232655116 seconds\n",
      "Average FPS:  106.29028869942036\n",
      "Total Images Processed :  112\n",
      "Total Time :  0.49348998069763184 seconds\n",
      "Average Time:  0.0044061605419431415 seconds\n",
      "Average FPS:  226.9549623716149\n",
      "Total Images Processed :  112\n",
      "Total Time :  0.28965330123901367 seconds\n",
      "Average Time:  0.0025861901896340506 seconds\n",
      "Average FPS:  386.66916455262765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Total Images': 112,\n",
       " 'tot_time': 0.28965330123901367,\n",
       " 'mean_time': 0.0025861901896340506,\n",
       " 'mean_fps': 386.66916455262765}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utils.device = \"cuda\"\n",
    "utils.benchmark_model(base_model, testloader, 100)\n",
    "utils.benchmark_model(quantized_base_model, testloader, 100)\n",
    "utils.benchmark_model(pruned_model, testloader, 100)\n",
    "utils.benchmark_model(quantized_pruned_model, testloader, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360d82b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.08092272355550509, 57.32794650320424)\n",
      "(0.0814350373019545, 56.96572861521315)\n",
      "(0.07692547515761258, 56.85427695736974)\n",
      "(0.07708757794411543, 56.32488158261354)\n"
     ]
    }
   ],
   "source": [
    "print(utils.test(base_model, testloader))\n",
    "print(utils.test(quantized_base_model, testloader))\n",
    "print(utils.test(pruned_model, testloader))\n",
    "print(utils.test(quantized_pruned_model, testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd02fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_input = torch.randn(1, 1, 96, 96)\n",
    "traced_base_model = torch.jit.trace(base_model, trace_input)\n",
    "trace_pruned_model = torch.jit.trace(pruned_model, trace_input)\n",
    "traced_quantized_base_model = torch.jit.trace(quantized_base_model, trace_input)\n",
    "traced_quantized_pruned_model = torch.jit.trace(quantized_pruned_model, trace_input)\n",
    "torch.jit.save(traced_base_model, os.path.join(\"base_model\", \"jit_traced.pth\"))\n",
    "torch.jit.save(trace_pruned_model, os.path.join(\"pruned_model\", \"jit_traced.pth\"))\n",
    "torch.jit.save(traced_quantized_base_model, os.path.join(\"base_model\", \"jit_traced_quantized.pth\"))\n",
    "torch.jit.save(traced_quantized_pruned_model, os.path.join(\"pruned_model\", \"jit_traced_quantized.pth\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPML_Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
